{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb84f2fd",
   "metadata": {},
   "source": [
    "# AutoGluon Tabular with Deep Learning Containers on SageMaker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a389c40c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|autogluon-tabular-containers|AutoGluon_Tabular_SageMaker_Containers.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "775e84ba",
   "metadata": {},
   "source": [
    "[AutoGluon](https://github.com/awslabs/autogluon) automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications. With just a few lines of code, you can train and deploy high-accuracy deep learning models on tabular, image, and text data.\n",
    "This example shows how to use AutoGluon-Tabular with Amazon SageMaker by applying [pre-built deep learning containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#autogluon-training-containers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4992c4f",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ea8f68a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:39.035856Z",
     "start_time": "2024-03-26T09:52:39.027591Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Ensure autogluon the most recent images information is available in SageMaker Python SDK\n",
    "# !pip install -q -U 'sagemaker>=2.126.0'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "path_root = \"/Users/elnath/004_deep_learning/SageMaker-Practical-Course\"\n",
    "conf = OmegaConf.load(f\"{path_root}/config_my.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:39.220409Z",
     "start_time": "2024-03-26T09:52:39.214645Z"
    }
   },
   "id": "74ebcbfcce044458",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faf25796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:40.082992Z",
     "start_time": "2024-03-26T09:52:39.396656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/elnath/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "from ag_model import (\n",
    "    AutoGluonSagemakerEstimator,\n",
    "    AutoGluonNonRepackInferenceModel,\n",
    "    AutoGluonSagemakerInferenceModel,\n",
    "    AutoGluonRealtimePredictor,\n",
    "    AutoGluonBatchPredictor,\n",
    ")\n",
    "from sagemaker import utils\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = conf.common.execution_role\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = f\"autogluon_sm/{utils.sagemaker_timestamp()}\"\n",
    "output_path = f\"s3://{bucket}/{s3_prefix}/output/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c006dc0",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "We'll be using the [Adult Census dataset](https://archive.ics.uci.edu/ml/datasets/adult) for this exercise. \n",
    "This data was extracted from the [1994 Census bureau database](http://www.census.gov/en.html) by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics), with the task being to predict if an individual person makes over 50K a year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d00074fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:40.250483Z",
     "start_time": "2024-03-26T09:52:40.081559Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a102722b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:40.252714Z",
     "start_time": "2024-03-26T09:52:40.232555Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"class\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f1218fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:40.993348Z",
     "start_time": "2024-03-26T09:52:40.241055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the data - needed for examples; in notebooks, S3 URL can be directly used for loading from S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/tabular/uci_adult/adult.data\",\n",
    "    \"data/adult.data\",\n",
    ")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/tabular/uci_adult/adult.test\",\n",
    "    \"data/adult.test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c242a5f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:41.275888Z",
     "start_time": "2024-03-26T09:52:40.994211Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/adult.data\", header=None, names=columns)\n",
    "df_train.to_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f2888ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:41.372375Z",
     "start_time": "2024-03-26T09:52:41.247448Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/adult.test\", header=None, skiprows=1, names=columns)\n",
    "df_test[\"class\"] = df_test[\"class\"].map(\n",
    "    {\n",
    "        \" <=50K.\": \" <=50K\",\n",
    "        \" >50K.\": \" >50K\",\n",
    "    }\n",
    ")\n",
    "df_test.to_csv(\"data/test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8213590f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b453761",
   "metadata": {},
   "source": [
    "Users can create their own training/inference scripts using [SageMaker Python SDK examples](https://sagemaker.readthedocs.io/en/stable/overview.html#prepare-a-training-script).\n",
    "The scripts we created allow to pass AutoGluon configuration as a YAML file (located in `data/config` directory).\n",
    "\n",
    "We are using [official AutoGluon Deep Learning Container images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#autogluon-training-containers) with custom training scripts (see `scripts/` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da167200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:41.455499Z",
     "start_time": "2024-03-26T09:52:41.371965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/elnath/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/elnath/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "ag = AutoGluonSagemakerEstimator(\n",
    "    role=role,\n",
    "    entry_point=\"scripts/tabular_train.py\",\n",
    "    region=region,\n",
    "    instance_count=1,\n",
    "    # instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_type=\"local\",\n",
    "    # conda에서의 sagemeaker SDK 버전이 낮아 1.0 미지원\n",
    "    framework_version=\"0.8\",\n",
    "    py_version=\"py39\",\n",
    "    base_job_name=\"autogluon-tabular-train\",\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69870f34",
   "metadata": {},
   "source": [
    "Upload the data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "204a60b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:52:42.937955Z",
     "start_time": "2024-03-26T09:52:41.421709Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "s3_prefix = f\"autogluon_sm/{utils.sagemaker_timestamp()}\"\n",
    "train_input = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"data\", \"train.csv\"), key_prefix=s3_prefix\n",
    ")\n",
    "eval_input = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"data\", \"test.csv\"), key_prefix=s3_prefix\n",
    ")\n",
    "config_input = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"config\", \"config-med.yaml\"), key_prefix=s3_prefix\n",
    ")\n",
    "\n",
    "# Provide inference script so the script repacking is not needed later\n",
    "# See more here: https://docs.aws.amazon.com/sagemaker/latest/dg/mlopsfaq.html\n",
    "# Q. Why do I see a repack step in my SageMaker pipeline?\n",
    "inference_script = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"scripts\", \"tabular_serve.py\"), key_prefix=s3_prefix\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fedb3195",
   "metadata": {},
   "source": [
    "### Fit The Model\n",
    "For local training set `instance_type` to local.\n",
    "\n",
    "For non-local training the recommended instance type is `ml.m5.2xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ea738",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-26T09:52:42.922299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: test-autogluon-image-1711446762-9abf\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-28c43:\n",
      "    command: train\n",
      "    container_name: u4na7hxy1w-algo-1-28c43\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-training:0.8-cpu-py39\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-28c43\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpvwf859g4/algo-1-28c43/input:/opt/ml/input\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpvwf859g4/algo-1-28c43/output:/opt/ml/output\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpvwf859g4/algo-1-28c43/output/data:/opt/ml/output/data\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpvwf859g4/model:/opt/ml/model\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmphhumkvot:/opt/ml/input/data/config\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpctd43xn1:/opt/ml/input/data/train\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpr1kfmenf:/opt/ml/input/data/test\n",
      "    - /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmp2yl63v7p:/opt/ml/input/data/serving\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/yp/836q9b2x1_n1fm3627j48ssc0000gn/T/tmpvwf859g4/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2024-03-26T18:52:48+09:00\" level=warning msg=\"a network with name sagemaker-local exists but was not created for project \\\"tmpvwf859g4\\\".\\nSet `external: true` to use an existing network\"\n",
      " Container u4na7hxy1w-algo-1-28c43  Creating\n",
      " Container u4na7hxy1w-algo-1-28c43  Created\n",
      "Attaching to u4na7hxy1w-algo-1-28c43\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,602 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,604 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,606 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,615 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,618 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,628 sagemaker_pytorch_container.training INFO     Invoking user training script.\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:50,793 botocore.credentials INFO     Found credentials in environment variables.\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,044 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,047 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,056 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,063 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,066 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,077 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,083 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,086 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,096 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\r\n",
      "u4na7hxy1w-algo-1-28c43  | 2024-03-26 09:52:51,099 sagemaker-training-toolkit INFO     Invoking user script\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | Training Env:\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | {\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"additional_framework_parameters\": {},\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"channel_input_dirs\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"config\": \"/opt/ml/input/data/config\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"train\": \"/opt/ml/input/data/train\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"test\": \"/opt/ml/input/data/test\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"serving\": \"/opt/ml/input/data/serving\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |     },\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"current_host\": \"algo-1-28c43\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"current_instance_group\": \"homogeneousCluster\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"current_instance_group_hosts\": [],\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"current_instance_type\": \"local\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"distribution_hosts\": [\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"algo-1-28c43\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |     ],\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"distribution_instance_groups\": [],\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"hosts\": [\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"algo-1-28c43\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |     ],\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"hyperparameters\": {},\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"input_config_dir\": \"/opt/ml/input/config\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"input_data_config\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"config\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |             \"TrainingInputMode\": \"File\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |         },\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"train\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |             \"TrainingInputMode\": \"File\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |         },\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"test\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |             \"TrainingInputMode\": \"File\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |         },\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"serving\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |             \"TrainingInputMode\": \"File\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |         }\r\n",
      "u4na7hxy1w-algo-1-28c43  |     },\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"input_dir\": \"/opt/ml/input\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"instance_groups\": [],\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"instance_groups_dict\": {},\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"is_hetero\": false,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"is_master\": true,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"is_modelparallel_enabled\": null,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"is_smddpmprun_installed\": false,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"job_name\": \"test-autogluon-image-1711446762-9abf\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"log_level\": 20,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"master_hostname\": \"algo-1-28c43\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"model_dir\": \"/opt/ml/model\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"module_dir\": \"s3://sagemaker-ap-northeast-2-688554574862/test-autogluon-image-1711446762-9abf/source/sourcedir.tar.gz\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"module_name\": \"tabular_train\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"network_interface_name\": \"eth0\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"num_cpus\": 16,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"num_gpus\": 0,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"num_neurons\": 0,\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"output_data_dir\": \"/opt/ml/output/data\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"output_dir\": \"/opt/ml/output\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"resource_config\": {\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"current_host\": \"algo-1-28c43\",\r\n",
      "u4na7hxy1w-algo-1-28c43  |         \"hosts\": [\r\n",
      "u4na7hxy1w-algo-1-28c43  |             \"algo-1-28c43\"\r\n",
      "u4na7hxy1w-algo-1-28c43  |         ]\r\n",
      "u4na7hxy1w-algo-1-28c43  |     },\r\n",
      "u4na7hxy1w-algo-1-28c43  |     \"user_entry_point\": \"tabular_train.py\"\r\n",
      "u4na7hxy1w-algo-1-28c43  | }\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | Environment variables:\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_HOSTS=[\"algo-1-28c43\"]\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_NETWORK_INTERFACE_NAME=eth0\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_HPS={}\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_USER_ENTRY_POINT=tabular_train.py\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_FRAMEWORK_PARAMS={}\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-28c43\",\"hosts\":[\"algo-1-28c43\"]}\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_INPUT_DATA_CONFIG={\"config\":{\"TrainingInputMode\":\"File\"},\"serving\":{\"TrainingInputMode\":\"File\"},\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CHANNELS=[\"config\",\"serving\",\"test\",\"train\"]\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CURRENT_HOST=algo-1-28c43\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CURRENT_INSTANCE_TYPE=local\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_INSTANCE_GROUPS=[]\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_INSTANCE_GROUPS_DICT={}\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_IS_HETERO=false\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_MODULE_NAME=tabular_train\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_LOG_LEVEL=20\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_INPUT_DIR=/opt/ml/input\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_OUTPUT_DIR=/opt/ml/output\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_NUM_CPUS=16\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_NUM_GPUS=0\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_NUM_NEURONS=0\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_MODEL_DIR=/opt/ml/model\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_MODULE_DIR=s3://sagemaker-ap-northeast-2-688554574862/test-autogluon-image-1711446762-9abf/source/sourcedir.tar.gz\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"/opt/ml/input/data/config\",\"serving\":\"/opt/ml/input/data/serving\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-28c43\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-28c43\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-28c43\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"config\":{\"TrainingInputMode\":\"File\"},\"serving\":{\"TrainingInputMode\":\"File\"},\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"test-autogluon-image-1711446762-9abf\",\"log_level\":20,\"master_hostname\":\"algo-1-28c43\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-688554574862/test-autogluon-image-1711446762-9abf/source/sourcedir.tar.gz\",\"module_name\":\"tabular_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-28c43\",\"hosts\":[\"algo-1-28c43\"]},\"user_entry_point\":\"tabular_train.py\"}\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_USER_ARGS=[]\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CHANNEL_CONFIG=/opt/ml/input/data/config\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CHANNEL_TRAIN=/opt/ml/input/data/train\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CHANNEL_TEST=/opt/ml/input/data/test\r\n",
      "u4na7hxy1w-algo-1-28c43  | SM_CHANNEL_SERVING=/opt/ml/input/data/serving\r\n",
      "u4na7hxy1w-algo-1-28c43  | PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | Invoking script with the following command:\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | /opt/conda/bin/python3.9 tabular_train.py\r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n",
      "u4na7hxy1w-algo-1-28c43  | \r\n"
     ]
    }
   ],
   "source": [
    "job_name = utils.unique_name_from_base(\"test-autogluon-image\")\n",
    "ag.fit(\n",
    "    {\n",
    "        \"config\": config_input,\n",
    "        \"train\": train_input,\n",
    "        \"test\": eval_input,\n",
    "        \"serving\": inference_script,\n",
    "    },\n",
    "    job_name=job_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4d57c00",
   "metadata": {},
   "source": [
    "### Model export\n",
    "\n",
    "AutoGluon models are portable: everything needed to deploy a trained model is in the tarball created by SageMaker.\n",
    "\n",
    "The artifact can be used locally, on EC2/ECS/EKS or served via SageMaker Inference."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(ag.model_data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "486077f46293e104",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5dccd",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {ag.model_data} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240c86b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!ls -alF model.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7633d597",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Endpoint Deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb4fb9ec",
   "metadata": {},
   "source": [
    "Upload the model we trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6eaa2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.unique_name_from_base(\"sagemaker-autogluon-serving-trained-model\")\n",
    "\n",
    "model_data = sagemaker_session.upload_data(\n",
    "    path=os.path.join(\".\", \"model.tar.gz\"), key_prefix=f\"{endpoint_name}/models\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4c9a075",
   "metadata": {},
   "source": [
    "Deploy remote or local endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6dd02",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\"\n",
    "# instance_type = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49533e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model = AutoGluonNonRepackInferenceModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.8\",\n",
    "    py_version=\"py39\",\n",
    "    instance_type=instance_type,\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"tabular_serve.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e333d24",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model.deploy(initial_instance_count=1, serializer=CSVSerializer(), instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(model.endpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5a1d909654069cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30190a41-71df-4eca-a616-a32ee9b5b50a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "predictor = AutoGluonRealtimePredictor(model.endpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52982b98",
   "metadata": {},
   "source": [
    "### Predict on unlabeled test data\n",
    "\n",
    "Remove target variable (`class`) from the data and get predictions for a sample of 100 rows using the deployed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85e3fab4",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:04.038437Z",
     "start_time": "2024-03-26T09:18:03.994213Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test.csv\")\n",
    "data = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c61ab16a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:06.735763Z",
     "start_time": "2024-03-26T09:18:05.411519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      pred   <=50K_proba   >50K_proba\n0    <=50K      0.997886     0.002114\n1    <=50K      0.819394     0.180606\n2    <=50K      0.673882     0.326118\n3     >50K      0.011298     0.988702\n4    <=50K      0.999636     0.000364\n..     ...           ...          ...\n95   <=50K      0.999180     0.000820\n96   <=50K      0.988200     0.011800\n97   <=50K      0.913842     0.086158\n98   <=50K      0.655580     0.344420\n99   <=50K      0.998832     0.001168\n\n[100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pred</th>\n      <th>&lt;=50K_proba</th>\n      <th>&gt;50K_proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;=50K</td>\n      <td>0.997886</td>\n      <td>0.002114</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;=50K</td>\n      <td>0.819394</td>\n      <td>0.180606</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;=50K</td>\n      <td>0.673882</td>\n      <td>0.326118</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&gt;50K</td>\n      <td>0.011298</td>\n      <td>0.988702</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;=50K</td>\n      <td>0.999636</td>\n      <td>0.000364</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>&lt;=50K</td>\n      <td>0.999180</td>\n      <td>0.000820</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>&lt;=50K</td>\n      <td>0.988200</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>&lt;=50K</td>\n      <td>0.913842</td>\n      <td>0.086158</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>&lt;=50K</td>\n      <td>0.655580</td>\n      <td>0.344420</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>&lt;=50K</td>\n      <td>0.998832</td>\n      <td>0.001168</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predictor.predict(data.drop(columns=\"class\"))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f7a8ab4",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:08.978105Z",
     "start_time": "2024-03-26T09:18:08.968229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     pred  actual\n0   <=50K   <=50K\n1   <=50K   <=50K\n2   <=50K    >50K\n3    >50K    >50K\n4   <=50K   <=50K",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pred</th>\n      <th>actual</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;=50K</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;=50K</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;=50K</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&gt;50K</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;=50K</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = preds[[\"pred\"]]\n",
    "p = p.join(data[\"class\"]).rename(columns={\"class\": \"actual\"})\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d9dcbe",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:09.866211Z",
     "start_time": "2024-03-26T09:18:09.851011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/100 are correct\n"
     ]
    }
   ],
   "source": [
    "print(f\"{(p.pred==p.actual).astype(int).sum()}/{len(p)} are correct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "084ee65e",
   "metadata": {},
   "source": [
    "### Cleanup Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdd315e3",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:17.917851Z",
     "start_time": "2024-03-26T09:18:15.462827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: autogluon-inference-2024-03-26-09-10-29-167\n",
      "INFO:sagemaker:Deleting endpoint with name: autogluon-inference-2024-03-26-09-10-29-167\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8197080",
   "metadata": {},
   "source": [
    "# Batch Transform\n",
    "\n",
    "Deploying a trained model to a hosted endpoint has been available in SageMaker since launch and is a great way to provide real-time predictions to a service like a website or mobile app. But, if the goal is to generate predictions from a trained model on a large dataset where minimizing latency isn’t a concern, then the batch transform functionality may be easier, more scalable, and more appropriate.\n",
    "\n",
    "[Read more about Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b607438",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:28.581537Z",
     "start_time": "2024-03-26T09:18:22.656173Z"
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.unique_name_from_base(\n",
    "    \"sagemaker-autogluon-batch_transform-trained-model\"\n",
    ")\n",
    "\n",
    "model_data = sagemaker_session.upload_data(\n",
    "    path=os.path.join(\".\", \"model.tar.gz\"), key_prefix=f\"{endpoint_name}/models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab078513",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c88630d4",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:18:40.051362Z",
     "start_time": "2024-03-26T09:18:40.023426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/elnath/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "model = AutoGluonSagemakerInferenceModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.8\",\n",
    "    py_version=\"py39\",\n",
    "    instance_type=instance_type,\n",
    "    entry_point=\"tabular_serve-batch.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    predictor_cls=AutoGluonBatchPredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "276fd30b",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:24:58.066275Z",
     "start_time": "2024-03-26T09:19:01.011972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/elnath/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-ap-northeast-2-688554574862/sagemaker-autogluon-batch_transform-trained-mod-1711444702-fd3d/models/model.tar.gz), script artifact (scripts), and dependencies ([]) into single tar.gz file located at s3://sagemaker-ap-northeast-2-688554574862/autogluon-inference-2024-03-26-09-19-01-205/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: autogluon-inference-2024-03-26-09-24-57-380\n"
     ]
    }
   ],
   "source": [
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=6,\n",
    "    max_concurrent_transforms=1,\n",
    "    output_path=output_path,\n",
    "    accept=\"application/json\",\n",
    "    assemble_with=\"Line\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9559625",
   "metadata": {},
   "source": [
    "Prepare data for batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8dde772",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:24:58.111548Z",
     "start_time": "2024-03-26T09:24:58.067529Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(f\"data/test.csv\")[:100].to_csv(\"data/test_no_header.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "355fabc8",
   "metadata": {},
   "source": [
    "Upload data to sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3dbd00",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:24:58.229448Z",
     "start_time": "2024-03-26T09:24:58.107772Z"
    }
   },
   "outputs": [],
   "source": [
    "test_input = transformer.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"data\", \"test_no_header.csv\"), key_prefix=s3_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b21b56f5",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:31:34.628109Z",
     "start_time": "2024-03-26T09:24:58.231816Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: autogluon-inference-2024-03-26-09-24-58-230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................\u001B[34m['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\u001B[0m\n",
      "\u001B[34mWarning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\u001B[0m\n",
      "\u001B[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,106 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,227 [INFO ] main org.pytorch.serve.ModelServer - \u001B[0m\n",
      "\u001B[34mTorchserve version: 0.7.1\u001B[0m\n",
      "\u001B[34mTS Home: /opt/conda/lib/python3.9/site-packages\u001B[0m\n",
      "\u001B[34mCurrent directory: /\u001B[0m\n",
      "\u001B[34mTemp directory: /home/model-server/tmp\u001B[0m\n",
      "\u001B[34mMetrics config path: /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml\u001B[0m\n",
      "\u001B[34mNumber of GPUs: 0\u001B[0m\n",
      "\u001B[34mNumber of CPUs: 8\u001B[0m\n",
      "\u001B[34mMax heap size: 7924 M\u001B[0m\n",
      "\u001B[34mPython executable: /opt/conda/bin/python3.9\u001B[0m\n",
      "\u001B[34mConfig file: /etc/sagemaker-ts.properties\u001B[0m\n",
      "\u001B[34mInference address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34mManagement address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34mMetrics address: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[34mModel Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[34mInitial Models: model=/opt/ml/model\u001B[0m\n",
      "\u001B[34mLog dir: /logs\u001B[0m\n",
      "\u001B[34mMetrics dir: /logs\u001B[0m\n",
      "\u001B[34mNetty threads: 0\u001B[0m\n",
      "\u001B[34mNetty client threads: 0\u001B[0m\n",
      "\u001B[34mDefault workers per model: 8\u001B[0m\n",
      "\u001B[34mBlacklist Regex: N/A\u001B[0m\n",
      "\u001B[34mMaximum Response Size: 6553500\u001B[0m\n",
      "\u001B[34mMaximum Request Size: 6553500\u001B[0m\n",
      "\u001B[34mLimit Maximum Image Pixels: true\u001B[0m\n",
      "\u001B[34mPrefer direct buffer: false\u001B[0m\n",
      "\u001B[34mAllowed Urls: [file://.*|http(s)?://.*]\u001B[0m\n",
      "\u001B[34mCustom python dependency for model allowed: false\u001B[0m\n",
      "\u001B[34mMetrics report format: prometheus\u001B[0m\n",
      "\u001B[34mEnable metrics API: true\u001B[0m\n",
      "\u001B[34mWorkflow Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[34mModel config: N/A\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,236 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,260 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,263 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,263 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,265 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,284 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,625 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,626 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,646 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[34mModel server started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,241 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,437 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,440 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:44.459388732910156|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,441 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:11.405742645263672|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,443 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:20.4|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,444 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29817.64453125|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,445 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1421.3359375|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,447 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:5.9|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,975 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,982 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,983 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]64\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,984 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,990 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,993 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,005 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,009 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456009\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,070 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]62\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,070 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,070 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,074 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,073 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]67\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,077 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,077 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]66\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,077 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,141 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,145 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,189 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,200 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,207 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,208 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]65\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,208 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,209 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,209 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,221 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,236 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,238 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456238\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,238 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,297 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,308 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,334 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,342 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]58\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,344 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,344 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,346 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,354 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,369 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456369\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,395 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,405 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]61\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,406 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,406 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,407 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,444 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,444 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456444\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,445 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,780 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,801 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]63\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,814 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,815 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456815\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,861 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,160 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,162 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,162 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,163 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1709\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2023\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:3904|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:3902|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,176 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,160 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,162 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,162 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,163 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1709\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2023\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:3904|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:3902|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,176 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,176 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:144|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,204 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,205 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1973\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:3934|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,232 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,233 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1742\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:3962|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,236 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:50|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,243 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,244 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2026\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:3972|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,247 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:77|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,275 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,276 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2077\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:4003|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:57|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1990\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:4047|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1473\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:4088|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,364 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:76|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,176 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:144|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,204 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,205 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1973\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:3934|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,232 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,233 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1742\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:3962|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,236 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:50|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,243 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,244 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2026\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:3972|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,247 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:77|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,275 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,276 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2077\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:4003|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:57|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1990\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:4047|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1473\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:4088|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,364 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:76|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,254 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.255.130:45938 \"GET /ping HTTP/1.1\" 200 13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,255 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,272 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:45940 \"GET /execution-parameters HTTP/1.1\" 404 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,273 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,321 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445460321\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,323 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,254 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.255.130:45938 \"GET /ping HTTP/1.1\" 200 13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,255 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,272 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:45940 \"GET /execution-parameters HTTP/1.1\" 404 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,273 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,321 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445460321\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,323 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,258 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:934.45|#ModelName:model,Level:Model|#hostname:bdb993703762,requestID:fb5dcb8a-9b78-4819-b64f-1661c6281ab0,timestamp:1711445461\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 936\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:45950 \"POST /invocations HTTP/1.1\" 200 945\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,260 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,258 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:934.45|#ModelName:model,Level:Model|#hostname:bdb993703762,requestID:fb5dcb8a-9b78-4819-b64f-1661c6281ab0,timestamp:1711445461\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 936\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:45950 \"POST /invocations HTTP/1.1\" 200 945\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,260 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[32m2024-03-26T09:31:00.277:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001B[0m\n",
      "\u001B[34m['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\u001B[0m\n",
      "\u001B[34mWarning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\u001B[0m\n",
      "\u001B[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001B[0m\n",
      "\u001B[35m['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\u001B[0m\n",
      "\u001B[35mWarning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\u001B[0m\n",
      "\u001B[35mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,106 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,227 [INFO ] main org.pytorch.serve.ModelServer - \u001B[0m\n",
      "\u001B[34mTorchserve version: 0.7.1\u001B[0m\n",
      "\u001B[34mTS Home: /opt/conda/lib/python3.9/site-packages\u001B[0m\n",
      "\u001B[34mCurrent directory: /\u001B[0m\n",
      "\u001B[34mTemp directory: /home/model-server/tmp\u001B[0m\n",
      "\u001B[34mMetrics config path: /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml\u001B[0m\n",
      "\u001B[34mNumber of GPUs: 0\u001B[0m\n",
      "\u001B[34mNumber of CPUs: 8\u001B[0m\n",
      "\u001B[34mMax heap size: 7924 M\u001B[0m\n",
      "\u001B[34mPython executable: /opt/conda/bin/python3.9\u001B[0m\n",
      "\u001B[34mConfig file: /etc/sagemaker-ts.properties\u001B[0m\n",
      "\u001B[34mInference address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34mManagement address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34mMetrics address: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[34mModel Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[34mInitial Models: model=/opt/ml/model\u001B[0m\n",
      "\u001B[34mLog dir: /logs\u001B[0m\n",
      "\u001B[34mMetrics dir: /logs\u001B[0m\n",
      "\u001B[34mNetty threads: 0\u001B[0m\n",
      "\u001B[34mNetty client threads: 0\u001B[0m\n",
      "\u001B[34mDefault workers per model: 8\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,106 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,227 [INFO ] main org.pytorch.serve.ModelServer - \u001B[0m\n",
      "\u001B[35mTorchserve version: 0.7.1\u001B[0m\n",
      "\u001B[35mTS Home: /opt/conda/lib/python3.9/site-packages\u001B[0m\n",
      "\u001B[35mCurrent directory: /\u001B[0m\n",
      "\u001B[35mTemp directory: /home/model-server/tmp\u001B[0m\n",
      "\u001B[35mMetrics config path: /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml\u001B[0m\n",
      "\u001B[35mNumber of GPUs: 0\u001B[0m\n",
      "\u001B[35mNumber of CPUs: 8\u001B[0m\n",
      "\u001B[35mMax heap size: 7924 M\u001B[0m\n",
      "\u001B[35mPython executable: /opt/conda/bin/python3.9\u001B[0m\n",
      "\u001B[35mConfig file: /etc/sagemaker-ts.properties\u001B[0m\n",
      "\u001B[35mInference address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[35mManagement address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[35mMetrics address: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[35mModel Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[35mInitial Models: model=/opt/ml/model\u001B[0m\n",
      "\u001B[35mLog dir: /logs\u001B[0m\n",
      "\u001B[35mMetrics dir: /logs\u001B[0m\n",
      "\u001B[35mNetty threads: 0\u001B[0m\n",
      "\u001B[35mNetty client threads: 0\u001B[0m\n",
      "\u001B[35mDefault workers per model: 8\u001B[0m\n",
      "\u001B[34mBlacklist Regex: N/A\u001B[0m\n",
      "\u001B[34mMaximum Response Size: 6553500\u001B[0m\n",
      "\u001B[34mMaximum Request Size: 6553500\u001B[0m\n",
      "\u001B[34mLimit Maximum Image Pixels: true\u001B[0m\n",
      "\u001B[34mPrefer direct buffer: false\u001B[0m\n",
      "\u001B[34mAllowed Urls: [file://.*|http(s)?://.*]\u001B[0m\n",
      "\u001B[34mCustom python dependency for model allowed: false\u001B[0m\n",
      "\u001B[34mMetrics report format: prometheus\u001B[0m\n",
      "\u001B[34mEnable metrics API: true\u001B[0m\n",
      "\u001B[34mWorkflow Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[34mModel config: N/A\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,236 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,260 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,263 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,263 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,265 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,284 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,625 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,626 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:54,646 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[35mBlacklist Regex: N/A\u001B[0m\n",
      "\u001B[35mMaximum Response Size: 6553500\u001B[0m\n",
      "\u001B[35mMaximum Request Size: 6553500\u001B[0m\n",
      "\u001B[35mLimit Maximum Image Pixels: true\u001B[0m\n",
      "\u001B[35mPrefer direct buffer: false\u001B[0m\n",
      "\u001B[35mAllowed Urls: [file://.*|http(s)?://.*]\u001B[0m\n",
      "\u001B[35mCustom python dependency for model allowed: false\u001B[0m\n",
      "\u001B[35mMetrics report format: prometheus\u001B[0m\n",
      "\u001B[35mEnable metrics API: true\u001B[0m\n",
      "\u001B[35mWorkflow Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[35mModel config: N/A\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,236 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,260 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,263 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,263 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,265 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,284 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,625 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,626 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:54,646 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[34mModel server started.\u001B[0m\n",
      "\u001B[35mModel server started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,241 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,437 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,440 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:44.459388732910156|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,441 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:11.405742645263672|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,443 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:20.4|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,444 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29817.64453125|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,445 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1421.3359375|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,447 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:5.9|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,241 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,437 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,440 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:44.459388732910156|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,441 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:11.405742645263672|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,443 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:20.4|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,444 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29817.64453125|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,445 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1421.3359375|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,447 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:5.9|#Level:Host|#hostname:bdb993703762,timestamp:1711445455\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,975 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,982 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,983 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]64\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,984 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,990 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:55,993 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,005 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,009 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456009\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,975 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,982 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,983 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]64\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,984 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,990 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:55,993 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,005 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,009 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456009\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,070 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]62\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,070 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,070 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,074 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,073 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]67\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,077 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,077 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]66\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,077 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,141 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,145 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,144 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,189 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,200 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,207 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,208 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]65\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,070 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]62\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,072 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,070 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,070 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,074 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,073 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]67\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,077 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,076 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,077 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]66\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,077 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,078 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,141 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,144 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,145 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,144 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,144 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,144 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,144 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456144\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,189 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,200 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,207 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,208 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]65\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,208 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,209 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,209 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,221 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,236 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,238 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456238\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,238 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,297 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,308 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,334 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,342 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]58\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,344 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,344 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,346 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,354 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,369 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456369\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,395 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,405 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]61\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,406 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,406 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,407 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,444 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,444 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456444\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,445 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,780 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,801 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]63\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,208 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,209 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,209 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,221 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,236 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,238 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456238\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,238 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,297 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,308 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,334 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,342 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]58\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,344 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,344 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,346 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,354 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,369 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456369\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,395 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,405 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]61\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,406 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,406 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,407 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,444 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,444 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456444\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,445 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,780 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.9/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,801 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]63\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.9.13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,814 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007.\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,815 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456815\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,802 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,814 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007.\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,815 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445456815\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:56,861 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:56,861 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,160 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,162 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,162 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,163 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1709\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2023\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:3904|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:3902|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,176 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,160 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,162 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,162 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,163 [WARN ] W-9002-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1709\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2023\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:3904|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,174 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:3902|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,176 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,176 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:144|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,204 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,205 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1973\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:3934|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,232 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,233 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1742\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:3962|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,236 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:50|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,243 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,244 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2026\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:3972|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,247 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:77|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,275 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,276 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2077\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:4003|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:57|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1990\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:4047|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1473\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:4088|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:30:58,364 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:76|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,176 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:144|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,204 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,205 [WARN ] W-9005-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1973\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:3934|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,208 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,232 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,233 [WARN ] W-9003-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1742\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,235 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:3962|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,236 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:50|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,243 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,244 [WARN ] W-9004-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2026\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,246 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:3972|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,247 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:77|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,275 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,276 [WARN ] W-9006-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2077\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:4003|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,278 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:57|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,317 [WARN ] W-9001-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1990\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:4047|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,319 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Found 1 mismatches between original and current metadata:\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,361 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - #011INFO: AutoGluon Python micro version mismatch (original=3.9.16, current=3.9.13)\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1473\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,363 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:4088|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[35m2024-03-26T09:30:58,364 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:76|#Level:Host|#hostname:bdb993703762,timestamp:1711445458\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,254 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.255.130:45938 \"GET /ping HTTP/1.1\" 200 13\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,255 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,272 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:45940 \"GET /execution-parameters HTTP/1.1\" 404 1\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,273 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,321 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445460321\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:00,323 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,254 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.255.130:45938 \"GET /ping HTTP/1.1\" 200 13\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,255 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,272 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:45940 \"GET /execution-parameters HTTP/1.1\" 404 1\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,273 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,321 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1711445460321\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:00,323 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,258 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:934.45|#ModelName:model,Level:Model|#hostname:bdb993703762,requestID:fb5dcb8a-9b78-4819-b64f-1661c6281ab0,timestamp:1711445461\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 936\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:45950 \"POST /invocations HTTP/1.1\" 200 945\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[34m2024-03-26T09:31:01,260 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,258 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:934.45|#ModelName:model,Level:Model|#hostname:bdb993703762,requestID:fb5dcb8a-9b78-4819-b64f-1661c6281ab0,timestamp:1711445461\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 936\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:45950 \"POST /invocations HTTP/1.1\" 200 945\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bdb993703762,timestamp:1711445460\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,259 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[35m2024-03-26T09:31:01,260 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:bdb993703762,timestamp:1711445461\u001B[0m\n",
      "\u001B[32m2024-03-26T09:31:00.277:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    test_input,\n",
    "    input_filter=\"$[:14]\",  # filter-out target variable\n",
    "    split_type=\"Line\",\n",
    "    content_type=\"text/csv\",\n",
    "    output_filter=\"$['class']\",  # keep only prediction class in the output\n",
    ")\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee44ccfb",
   "metadata": {},
   "source": [
    "Download batch transform outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e07ff698",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:31:35.890097Z",
     "start_time": "2024-03-26T09:31:34.628629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-northeast-2-688554574862/autogluon_sm/2024-03-26-09-02-49-026/output/test_no_header.csv.out to ./test_no_header.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {transformer.output_path[:-1]}/test_no_header.csv.out ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac3b0953",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:31:35.952206Z",
     "start_time": "2024-03-26T09:31:35.892956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    preds  actual\n0   <=50K   <=50K\n1   <=50K   <=50K\n2   <=50K    >50K\n3    >50K    >50K\n4   <=50K   <=50K",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preds</th>\n      <th>actual</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;=50K</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;=50K</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;=50K</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&gt;50K</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;=50K</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.concat(\n",
    "    [\n",
    "        pd.read_json(\"test_no_header.csv.out\", orient=\"index\")\n",
    "        .sort_index()\n",
    "        .rename(columns={0: \"preds\"}),\n",
    "        pd.read_csv(\"data/test.csv\")[[\"class\"]].iloc[:100].rename(columns={\"class\": \"actual\"}),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7150fa02",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-26T09:33:10.919013Z",
     "start_time": "2024-03-26T09:33:10.901658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/100 are correct\n"
     ]
    }
   ],
   "source": [
    "print(f\"{(p.preds==p.actual).astype(int).sum()}/{len(p)} are correct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd3e9e5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial we successfully trained an AutoGluon model and explored a few options how to deploy it using SageMaker. Any of the sections of this tutorial (training/endpoint inference/batch inference) can be used independently (i.e. train locally, deploy to SageMaker, or vice versa).\n",
    "\n",
    "Next steps:\n",
    "* [Learn more](https://auto.gluon.ai) about AutoGluon, explore [tutorials](https://auto.gluon.ai/stable/tutorials/index.html).\n",
    "* Explore [SageMaker inference documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "name": "venv_sagemaker_39",
   "language": "python",
   "display_name": "venv_sagemaker_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
